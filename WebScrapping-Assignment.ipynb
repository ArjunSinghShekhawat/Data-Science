{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b7151-1fd9-4d06-baea-8ddf4e39a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "# Q2. What are the different methods used for Web Scraping?\n",
    "# Q3. What is Beautiful Soup? Why is it used?\n",
    "# Q4. Why is flask used in this Web Scraping project?\n",
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Q1. Web scraping is the automated process of extracting data from websites. It's used to gather information, monitor websites for changes, and analyze data. Three areas where web scraping is used are:\n",
    "   1. Business and Market Research: Collecting competitor data.\n",
    "   2. Content Aggregation: Gathering news articles or product listings.\n",
    "   3. Data Science: Obtaining datasets for analysis.\n",
    "\n",
    "Q2. Different web scraping methods include using libraries like Beautiful Soup and Scrapy, or utilizing APIs provided by websites. Some also use headless browsers with tools like Selenium for dynamic web content.\n",
    "\n",
    "Q3. Beautiful Soup is a Python library used for parsing HTML and XML documents. It's used to extract specific data from web pages, navigate through the HTML structure, and scrape the desired information.\n",
    "\n",
    "Q4. Flask is used in web scraping projects to create web applications or APIs that can present the scraped data in a user-friendly way. It helps in building a frontend for data visualization and interaction.\n",
    "\n",
    "Q5. AWS services used in a web scraping project can include:\n",
    "   - EC2 (Elastic Compute Cloud): For hosting web scraping scripts on virtual machines.\n",
    "   - S3 (Simple Storage Service): For storing scraped data.\n",
    "   - Lambda: To trigger scraping tasks periodically.\n",
    "   - DynamoDB: For storing metadata or managing job queues.\n",
    "   - CloudWatch: To monitor and manage resources.\n",
    "   - API Gateway: To expose scraped data through APIs.\n",
    "   - RDS (Relational Database Service): For structured data storage and analysis.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
