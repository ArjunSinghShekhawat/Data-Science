{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe47e1-00d3-4670-b0ab-6bdd828c0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "Q7: What is regularization in machine learning, and how can it be\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d437f41-26fd-4cb1-aad8-c2abcbe7a2ab",
   "metadata": {},
   "source": [
    "Q1: Overfitting occurs when a machine learning model learns the training data too well but performs poorly on new data. Underfitting happens when a model is too simple to capture the underlying patterns in the data. Overfitting can lead to poor generalization, while underfitting results in low model performance. Techniques to mitigate overfitting include reducing model complexity, using more data, and applying regularization. Underfitting can be addressed by increasing model complexity and gathering more relevant features.\n",
    "\n",
    "Q2: To reduce overfitting, you can:\n",
    "- Use simpler models.\n",
    "- Increase the amount of training data.\n",
    "- Apply regularization techniques like L1 or L2 regularization.\n",
    "- Use feature selection and engineering.\n",
    "- Implement early stopping during training.\n",
    "\n",
    "Q3: Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It can happen in scenarios where the model lacks complexity to represent the relationships present, such as in high-dimensional data, complex patterns, or when features are not informative enough.\n",
    "\n",
    "Q4: The bias-variance tradeoff is a fundamental concept in machine learning. Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. Variance is the error introduced by the model's sensitivity to small fluctuations in the training data. The relationship is that as bias decreases, variance increases, and vice versa. Finding the right balance is crucial for optimal model performance.\n",
    "\n",
    "Q5: Common methods for detecting overfitting and underfitting include:\n",
    "Cross-validation: Assess model performance on different subsets of the data.\n",
    "Learning curves: Plot training and validation error as a function of sample size.\n",
    "Regularization techniques: Observe the impact of regularization parameters.\n",
    "Visual inspection: Analyze model performance through visualization.\n",
    "\n",
    "You can determine overfitting if the training error is significantly lower than the validation error, and underfitting if both errors are high.\n",
    "\n",
    "Q6: In machine learning, bias refers to the error introduced by simplifying a model too much, resulting in high training error. High bias models are typically underfitted. Variance, on the other hand, represents the error due to a model's sensitivity to the training data, leading to high validation error. High variance models are often overfitted. High bias models are too simple, and high variance models are too complex.\n",
    "\n",
    "For example, a linear regression model may have high bias and underfit when trying to predict a non-linear relationship, while a complex neural network with many layers may have high variance and overfit when training data is limited.\n",
    "\n",
    "Q7: Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term to the model's loss function. It discourages large coefficients and overly complex models. Regularization methods, such as L1 (Lasso) and L2 (Ridge), help improve model generalization by balancing model complexity and fitting to the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
